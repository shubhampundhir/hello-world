{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO8R/S0rrg0T1Y2JFEeEPsa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhampundhir/hello-world/blob/master/RoadSegmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvNTZ4ZEtAMr",
        "outputId": "1776345e-c720-45b1-eaf1-745de8e611a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/VainF/DeepLabV3Plus-Pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgS4ytivtZvY",
        "outputId": "a073e070-e969-4169-9b5f-cf00f0338a0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DeepLabV3Plus-Pytorch'...\n",
            "remote: Enumerating objects: 705, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 705 (delta 25), reused 30 (delta 14), pack-reused 653\u001b[K\n",
            "Receiving objects: 100% (705/705), 8.26 MiB | 23.30 MiB/s, done.\n",
            "Resolving deltas: 100% (380/380), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd C:\\Users\\Shubham Pundhir\\Documents\\GitHub\\DeepLabV3Plus-Pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjDzetgSuB4f",
        "outputId": "1c8894a0-e925-4ce4-a16d-49b8e66bbef8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: cd: too many arguments\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd DeepLabV3Plus-Pytorch/"
      ],
      "metadata": {
        "id": "wjX90995uhoR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NEPONphvFBQ",
        "outputId": "4b053825-a5dd-476c-e9f6-8d04d2c25a85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepLabV3Plus-Pytorch  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/DeepLabV3Plus-Pytorch/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqDdH37zvGVX",
        "outputId": "af7de0bc-6229-416c-fc13-d1a379346a69"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DeepLabV3Plus-Pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXJfdkeEv2T0",
        "outputId": "a6ef406b-cfd0-4c96-e807-23e507526c25"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets  main.py  network     README.md\t samples\n",
            "LICENSE   metrics  predict.py  requirements.txt  utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUDuzs4pv5_6",
        "outputId": "0b9261fd-2f08-4aa3-d70e-7ed390934777"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: predict.py\n",
            "       [-h]\n",
            "       --input\n",
            "       INPUT\n",
            "       [--dataset {voc,cityscapes}]\n",
            "       [--model {deeplabv3_hrnetv2_32,deeplabv3_hrnetv2_48,deeplabv3_mobilenet,deeplabv3_resnet101,deeplabv3_resnet50,deeplabv3_xception,deeplabv3plus_hrnetv2_32,deeplabv3plus_hrnetv2_48,deeplabv3plus_mobilenet,deeplabv3plus_resnet101,deeplabv3plus_resnet50,deeplabv3plus_xception}]\n",
            "       [--separable_conv]\n",
            "       [--output_stride {8,16}]\n",
            "       [--save_val_results_to SAVE_VAL_RESULTS_TO]\n",
            "       [--crop_val]\n",
            "       [--val_batch_size VAL_BATCH_SIZE]\n",
            "       [--crop_size CROP_SIZE]\n",
            "       [--ckpt CKPT]\n",
            "       [--gpu_id GPU_ID]\n",
            "predict.py: error: the following arguments are required: --input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install visdom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ8F85yrwky_",
        "outputId": "54113032-9411-42df-96c9-885a9254cdc3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting visdom\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.10/dist-packages (from visdom) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom) (1.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from visdom) (2.31.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom) (6.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from visdom) (1.16.0)\n",
            "Collecting jsonpatch (from visdom)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom) (1.6.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from visdom) (3.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from visdom) (9.4.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch->visdom)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->visdom) (2023.7.22)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408195 sha256=f89d52ce855b8e6b0b2e08aa4dd16db7b7b44639cb107653c6811ac9da4f4191\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/29/49/5bed207bac4578e4d2c0c5fc0226bfd33a7e2953ea56356855\n",
            "Successfully built visdom\n",
            "Installing collected packages: jsonpointer, jsonpatch, visdom\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 visdom-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python predict.py --input /content/DeepLabV3Plus-Pytorch/samples/1_image.png --dataset cityscapes --model deeplabv3plus_mobilenet --ckpt checkpoints/best_deeplabv3plus_mobilenet_cityscapes_os16.pth --save_val_results_to test_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0gkQN6sw4N1",
        "outputId": "78eb1d4b-c7c3-4aea-d3b7-fad851b5adbe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "[!] Retrain\n",
            "100% 1/1 [00:01<00:00,  1.43s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/DeepLabV3Plus-Pytorch/predict.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYglRsU4w-3y",
        "outputId": "0f50c537-a762-42c3-9090-ed14e1c1f32e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from torch.utils.data import dataset\n",
            "from tqdm import tqdm\n",
            "import network\n",
            "import utils\n",
            "import os\n",
            "import random\n",
            "import argparse\n",
            "import numpy as np\n",
            "\n",
            "from torch.utils import data\n",
            "from datasets import VOCSegmentation, Cityscapes, cityscapes\n",
            "from torchvision import transforms as T\n",
            "from metrics import StreamSegMetrics\n",
            "\n",
            "import torch\n",
            "import torch.nn as nn\n",
            "\n",
            "from PIL import Image\n",
            "import matplotlib\n",
            "import matplotlib.pyplot as plt\n",
            "from glob import glob\n",
            "\n",
            "def get_argparser():\n",
            "    parser = argparse.ArgumentParser()\n",
            "\n",
            "    # Datset Options\n",
            "    parser.add_argument(\"--input\", type=str, required=True,\n",
            "                        help=\"path to a single image or image directory\")\n",
            "    parser.add_argument(\"--dataset\", type=str, default='voc',\n",
            "                        choices=['voc', 'cityscapes'], help='Name of training set')\n",
            "\n",
            "    # Deeplab Options\n",
            "    available_models = sorted(name for name in network.modeling.__dict__ if name.islower() and \\\n",
            "                              not (name.startswith(\"__\") or name.startswith('_')) and callable(\n",
            "                              network.modeling.__dict__[name])\n",
            "                              )\n",
            "\n",
            "    parser.add_argument(\"--model\", type=str, default='deeplabv3plus_mobilenet',\n",
            "                        choices=available_models, help='model name')\n",
            "    parser.add_argument(\"--separable_conv\", action='store_true', default=False,\n",
            "                        help=\"apply separable conv to decoder and aspp\")\n",
            "    parser.add_argument(\"--output_stride\", type=int, default=16, choices=[8, 16])\n",
            "\n",
            "    # Train Options\n",
            "    parser.add_argument(\"--save_val_results_to\", default=None,\n",
            "                        help=\"save segmentation results to the specified dir\")\n",
            "\n",
            "    parser.add_argument(\"--crop_val\", action='store_true', default=False,\n",
            "                        help='crop validation (default: False)')\n",
            "    parser.add_argument(\"--val_batch_size\", type=int, default=4,\n",
            "                        help='batch size for validation (default: 4)')\n",
            "    parser.add_argument(\"--crop_size\", type=int, default=513)\n",
            "\n",
            "    \n",
            "    parser.add_argument(\"--ckpt\", default=None, type=str,\n",
            "                        help=\"resume from checkpoint\")\n",
            "    parser.add_argument(\"--gpu_id\", type=str, default='0',\n",
            "                        help=\"GPU ID\")\n",
            "    return parser\n",
            "\n",
            "def main():\n",
            "    opts = get_argparser().parse_args()\n",
            "    if opts.dataset.lower() == 'voc':\n",
            "        opts.num_classes = 21\n",
            "        decode_fn = VOCSegmentation.decode_target\n",
            "    elif opts.dataset.lower() == 'cityscapes':\n",
            "        opts.num_classes = 19\n",
            "        decode_fn = Cityscapes.decode_target\n",
            "\n",
            "    os.environ['CUDA_VISIBLE_DEVICES'] = opts.gpu_id\n",
            "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
            "    print(\"Device: %s\" % device)\n",
            "\n",
            "    # Setup dataloader\n",
            "    image_files = []\n",
            "    if os.path.isdir(opts.input):\n",
            "        for ext in ['png', 'jpeg', 'jpg', 'JPEG']:\n",
            "            files = glob(os.path.join(opts.input, '**/*.%s'%(ext)), recursive=True)\n",
            "            if len(files)>0:\n",
            "                image_files.extend(files)\n",
            "    elif os.path.isfile(opts.input):\n",
            "        image_files.append(opts.input)\n",
            "    \n",
            "    # Set up model (all models are 'constructed at network.modeling)\n",
            "    model = network.modeling.__dict__[opts.model](num_classes=opts.num_classes, output_stride=opts.output_stride)\n",
            "    if opts.separable_conv and 'plus' in opts.model:\n",
            "        network.convert_to_separable_conv(model.classifier)\n",
            "    utils.set_bn_momentum(model.backbone, momentum=0.01)\n",
            "    \n",
            "    if opts.ckpt is not None and os.path.isfile(opts.ckpt):\n",
            "        # https://github.com/VainF/DeepLabV3Plus-Pytorch/issues/8#issuecomment-605601402, @PytaichukBohdan\n",
            "        checkpoint = torch.load(opts.ckpt, map_location=torch.device('cpu'))\n",
            "        model.load_state_dict(checkpoint[\"model_state\"])\n",
            "        model = nn.DataParallel(model)\n",
            "        model.to(device)\n",
            "        print(\"Resume model from %s\" % opts.ckpt)\n",
            "        del checkpoint\n",
            "    else:\n",
            "        print(\"[!] Retrain\")\n",
            "        model = nn.DataParallel(model)\n",
            "        model.to(device)\n",
            "\n",
            "    #denorm = utils.Denormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # denormalization for ori images\n",
            "\n",
            "    if opts.crop_val:\n",
            "        transform = T.Compose([\n",
            "                T.Resize(opts.crop_size),\n",
            "                T.CenterCrop(opts.crop_size),\n",
            "                T.ToTensor(),\n",
            "                T.Normalize(mean=[0.485, 0.456, 0.406],\n",
            "                                std=[0.229, 0.224, 0.225]),\n",
            "            ])\n",
            "    else:\n",
            "        transform = T.Compose([\n",
            "                T.ToTensor(),\n",
            "                T.Normalize(mean=[0.485, 0.456, 0.406],\n",
            "                                std=[0.229, 0.224, 0.225]),\n",
            "            ])\n",
            "    if opts.save_val_results_to is not None:\n",
            "        os.makedirs(opts.save_val_results_to, exist_ok=True)\n",
            "    with torch.no_grad():\n",
            "        model = model.eval()\n",
            "        for img_path in tqdm(image_files):\n",
            "            ext = os.path.basename(img_path).split('.')[-1]\n",
            "            img_name = os.path.basename(img_path)[:-len(ext)-1]\n",
            "            img = Image.open(img_path).convert('RGB')\n",
            "            img = transform(img).unsqueeze(0) # To tensor of NCHW\n",
            "            img = img.to(device)\n",
            "            \n",
            "            pred = model(img).max(1)[1].cpu().numpy()[0] # HW\n",
            "            colorized_preds = decode_fn(pred).astype('uint8')\n",
            "            colorized_preds = Image.fromarray(colorized_preds)\n",
            "            if opts.save_val_results_to:\n",
            "                colorized_preds.save(os.path.join(opts.save_val_results_to, img_name+'.png'))\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    main()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import dataset\n",
        "from tqdm import tqdm\n",
        "import network\n",
        "import utils\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils import data\n",
        "from datasets import VOCSegmentation, Cityscapes, cityscapes\n",
        "from torchvision import transforms as T\n",
        "from metrics import StreamSegMetrics\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from PIL import Image\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from glob import glob\n",
        "\n",
        "def get_argparser():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Datset Options\n",
        "    parser.add_argument(\"--input\", type=str, required=True,\n",
        "                        help=\"path to a single image or image directory\")\n",
        "    parser.add_argument(\"--dataset\", type=str, default='voc',\n",
        "                        choices=['voc', 'cityscapes'], help='Name of training set')\n",
        "\n",
        "    # Deeplab Options\n",
        "    available_models = sorted(name for name in network.modeling.__dict__ if name.islower() and \\\n",
        "                              not (name.startswith(\"__\") or name.startswith('_')) and callable(\n",
        "                              network.modeling.__dict__[name])\n",
        "                              )\n",
        "\n",
        "    parser.add_argument(\"--model\", type=str, default='deeplabv3plus_mobilenet',\n",
        "                        choices=available_models, help='model name')\n",
        "    parser.add_argument(\"--separable_conv\", action='store_true', default=False,\n",
        "                        help=\"apply separable conv to decoder and aspp\")\n",
        "    parser.add_argument(\"--output_stride\", type=int, default=16, choices=[8, 16])\n",
        "\n",
        "    # Train Options\n",
        "    parser.add_argument(\"--save_val_results_to\", default=None,\n",
        "                        help=\"save segmentation results to the specified dir\")\n",
        "\n",
        "    parser.add_argument(\"--crop_val\", action='store_true', default=False,\n",
        "                        help='crop validation (default: False)')\n",
        "    parser.add_argument(\"--val_batch_size\", type=int, default=4,\n",
        "                        help='batch size for validation (default: 4)')\n",
        "    parser.add_argument(\"--crop_size\", type=int, default=513)\n",
        "\n",
        "\n",
        "    parser.add_argument(\"--ckpt\", default=None, type=str,\n",
        "                        help=\"resume from checkpoint\")\n",
        "    parser.add_argument(\"--gpu_id\", type=str, default='0',\n",
        "                        help=\"GPU ID\")\n",
        "    return parser\n",
        "\n",
        "def main():\n",
        "    opts = get_argparser().parse_args()\n",
        "    if opts.dataset.lower() == 'voc':\n",
        "        opts.num_classes = 21\n",
        "        decode_fn = VOCSegmentation.decode_target\n",
        "    elif opts.dataset.lower() == 'cityscapes':\n",
        "        opts.num_classes = 19\n",
        "        decode_fn = Cityscapes.decode_target\n",
        "\n",
        "    os.environ['CUDA_VISIBLE_DEVICES'] = opts.gpu_id\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"Device: %s\" % device)\n",
        "\n",
        "    # Setup dataloader\n",
        "    image_files = []\n",
        "    if os.path.isdir(opts.input):\n",
        "        for ext in ['png', 'jpeg', 'jpg', 'JPEG']:\n",
        "            files = glob(os.path.join(opts.input, '**/*.%s'%(ext)), recursive=True)\n",
        "            if len(files)>0:\n",
        "                image_files.extend(files)\n",
        "    elif os.path.isfile(opts.input):\n",
        "        image_files.append(opts.input)\n",
        "\n",
        "    # Set up model (all models are 'constructed at network.modeling)\n",
        "    model = network.modeling.__dict__[opts.model](num_classes=opts.num_classes, output_stride=opts.output_stride)\n",
        "    if opts.separable_conv and 'plus' in opts.model:\n",
        "        network.convert_to_separable_conv(model.classifier)\n",
        "    utils.set_bn_momentum(model.backbone, momentum=0.01)\n",
        "\n",
        "    if opts.ckpt is not None and os.path.isfile(opts.ckpt):\n",
        "        # https://github.com/VainF/DeepLabV3Plus-Pytorch/issues/8#issuecomment-605601402, @PytaichukBohdan\n",
        "        checkpoint = torch.load(opts.ckpt, map_location=torch.device('cpu'))\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        model = nn.DataParallel(model)\n",
        "        model.to(device)\n",
        "        print(\"Resume model from %s\" % opts.ckpt)\n",
        "        del checkpoint\n",
        "    else:\n",
        "        print(\"[!] Retrain\")\n",
        "        model = nn.DataParallel(model)\n",
        "        model.to(device)\n",
        "\n",
        "    #denorm = utils.Denormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # denormalization for ori images\n",
        "\n",
        "    if opts.crop_val:\n",
        "        transform = T.Compose([\n",
        "                T.Resize(opts.crop_size),\n",
        "                T.CenterCrop(opts.crop_size),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "    else:\n",
        "        transform = T.Compose([\n",
        "                T.ToTensor(),\n",
        "                T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225]),\n",
        "            ])\n",
        "    if opts.save_val_results_to is not None:\n",
        "        os.makedirs(opts.save_val_results_to, exist_ok=True)\n",
        "    with torch.no_grad():\n",
        "        model = model.eval()\n",
        "        for img_path in tqdm(image_files):\n",
        "            ext = os.path.basename(img_path).split('.')[-1]\n",
        "            img_name = os.path.basename(img_path)[:-len(ext)-1]\n",
        "            img = Image.open(img_path).convert('RGB')\n",
        "            img = transform(img).unsqueeze(0) # To tensor of NCHW\n",
        "            img = img.to(device)\n",
        "\n",
        "            pred = model(img).max(1)[1].cpu().numpy()[0] # HW\n",
        "            colorized_preds = decode_fn(pred).astype('uint8')\n",
        "            colorized_preds = Image.fromarray(colorized_preds)\n",
        "            if opts.save_val_results_to:\n",
        "                colorized_preds.save(os.path.join(opts.save_val_results_to, img_name+'.png'))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "I6dE_VZPyd06"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}